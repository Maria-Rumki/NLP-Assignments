# NLP_Assignment
This repository is for submitting homework for AT82.05 Artificial Intelligence:NLU at Asian Institute of Technology

## Paper reading assignment
## 1. Improving Chinese Grammatical Error Detection via Data augmentation by Conditional Error Generation
### Tianchi Yue, Shulin Liu, Huihui Cai, Tao Yang, Shengkang Song, Tinghao Yu

| Problem  | How to detect grammatical errors in Chinese texts.|
| --- | --- |
| Proposal | For producing Chinese grammatical errors, they suggest the novel Conditional Non-Autoregressive Error Generation (CNEG) model. |
| Objective | Generate high-quality grammatical errors to improve the performance of CGED models. |
| Datasets | Public datasets from CGED tasks (Lee et al., 2016; Rao et al., 2017, 2018, 2020). |
|          | Select 2016, 2017, 2018 and 2020 training dataset as their training dataset. |
| Key Related Work  | 1. Improving Grammatical Error Correction with Machine Translation Pairs (Zhou et al., 2020) -> They Consider that the Neural Machine Translation (NMT) model is much superior to the Statistical Machine Translation (SMT) model, then use the NMT model to produce right and incorrect sentences, respectively.|
|                   | 2. Improving Grammatical Error Correction Models with Purpose-Built Adversarial Examples (Wang and Zheng, 2020) -> By using a seq2seq model, the most vulnerable tokens are first identified. These tokens are subsequently replaced with training dataset grammatical errors.|
| Solution  | They build their generative model around a BERT encoder with a nonautoregressive decoding layer to produce context-dependent mistakes. They include the original span in their generative model, allowing it to forecast the incorrect span given the original span. Additionally, they include a filtering technique to remove error-free spans.|
| Results  | Achieves better performance than all compared data augmentation methods on the CGED-2018 and CGED-2020 benchmarks. |
| In term of models   | The CNEG model can generate errors that do not appear in the training dataset. |
| In term of measurement  | The perplexities of generated spans are measured using a BERT that has already been trained. |


## 2. VISUALIZING AND UNDERSTANDING RECURRENT NETWORKS
### Author Andrej Karpathy, Justin Johnson, Li Fei-Fei

| Problem  | 1.What is the performance of Long Short-Term Memory (LSTM) models in capturing long-range dependencies such as line lengths, quotes, and brackets, and how can interpretable cells within the LSTM architecture be identified and analyzed to improve LSTM models' performance? |
| --- | --- |
| Key Related Work | The related work discusses the success of Long Short Term Memory networks (LSTM) in sequence learning tasks and various proposals for improving RNN architectures. The authors focus on studying the properties of LSTM representations and predicting individual error types. Their work complements previous studies on long-term interactions in recurrent networks and draws inspiration from object detection analysis. |
| Datasets | Penn Treebank dataset Marcus et al. (1993). |
|          | Hutter Prize 100MB of Wikipedia dataset Hutter (2012). |
| Solution  | The authors of this study used character-level language models to explore the predictions and learned representations of long short-term memory networks (LSTMs) on real-world data. Their experiments, which included qualitative visualization and cell activation statistics, demonstrated that RNNs are able to learn powerful and often interpretable long-range interactions on real-world data.Also conducted an error analysis, which allowed them to break down cross entropy loss into several interpretable categories and identify the sources of remaining limitations in the model. |
| Results  | The study revealed that increasing the size of the model significantly reduced errors in the n-gram category, indicating that more advanced structural changes might be necessary to address the remaining errors. |

## 3. Semi-supervised sequence tagging with bidirectional language models
### Matthew E. Peters, Waleed Ammar, Chandra Bhagavatula, Russell Power

| Problem  | What is the proposed approach for adding pretrained context embeddings to NLP systems and how is it different from existing approaches?|
| --- | --- |
| Proposal | adding pretrained context embeddings from bidirectional language models to NLP systems to produce context-sensitive representations. |
| Objective | To demonstrate a semi-supervised approach for improving the performance of neural network architectures for natural language processing (NLP) tasks such as named entity recognition (NER) and chunking. |
| Datasets | d entity recognition (NER) and chunking |
| Key Related Work  | 1. Unlabeled data: The TagLM method is inspired by pre-trained word embeddings in supervised sequence tagging models and is similar to Li and McCallum's (2005) probabilistic generative model. Other semi-supervised learning methods for structured prediction problems include co-training, expectation maximization, structural learning, and maximum discriminant functions. LM embeddings are related to methods for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks. Dai and Le (2015) pre-trained LSTMs using language models and sequence autoencoders for classification tasks, whereas TagLM uses unlabeled data to learn token-in-context embeddings.|
|                   | 2. Interpreting RNN states. The article cites previous research on interpreting RNN activations, including Linzen et al. (2016), who demonstrated that LSTM units could learn to predict singular-plural distinctions, and Karpathy et al. (2015), who visualized character level LSTM states and identified that individual cells capture long-range dependencies such as line lengths, quotes, and brackets. The authors state that their work complements these studies by demonstrating that LM states are useful for downstream tasks as a means of interpreting what they learn.|
|                   | 3. Other sequence tagging models: Bidirectional RNN models are currently the state-of-the-art for sequence tagging problems. Other sequence tagging models have also been proposed, but it is uncertain if LM embeddings could be effectively used as additional features in these models due to potential limitations in their complexity. |
| Solution  | The approach involves incorporating pre-trained context embeddings from bidirectional language models into the NLP systems. This method is applied to sequence labeling tasks, specifically named entity recognition (NER) and chunking. |
| Results  | The paper demonstrates that the addition of pre-trained context embeddings improves the performance of the NLP system and achieves state-of-the-art results on the two standard datasets for NER and chunking. This approach outperforms previous systems that use other forms of transfer or joint learning with additional labeled data and task-specific gazetteers.|


## 4 PPT: Pre-trained Prompt Tuning for Few-shot Learning

Paper link: https://arxiv.org/pdf/2109.04332.pdf

| Aim | Gu et al. proposed pre-trained prompts by adding soft prompts during the pre-training stage in order to achieve a better initialization for downstream tasks. | 
| ------- | --- | 
| Background | Prompts for pre-trained language models (PLMs) has been proved very effective since they worked as the connection between pre-training tasks and various downstream tasks. However, prompt tuning freezes PLMs and tunes soft prompts only, thus it is able to provide an efficient solution for adapting large-scale PLMs to downstream tasks. Though prompt tuning performs very well with sufficient downstream data, it is much worse under few-shot learning settings which may hinder the performance of prompt tuning in different applications. In this paper, Gu et al. proposed pre-trained prompt tuning (PPT) to address this limitation. | 
| Datasets | They mostly experimented with English and Chinese datasets, where they presented the results of the T5 model (from small size to XXL) for full-model tuning (FT) and the results of PPT and other baselines under prompt tuning (PT). To be more spefic, they used SST-2, RACE-m, BoolQ, CB datasets for English tasks and CCPM, C<sup>3</sup>, LCQMC etc datasets for Chinese tasks. | 
| Methods | For this work, they pre-trained prompts and used the pre-trained prompts for specific tasks. Given an input sentence and its label, a pattern mapping is first applied to convert the input into a new sequence, whereas the new sequence adds some prompt tokens as hints and preserves the masked tokens at the same time so that the PLMs can predict tokens at the masked positions. Then a verbalizer is used to map the true labels to some label tokens. The classification task can be represented by a pattern-verbalizer pair of new sequences and the label tokens. <br> Moreover, they assumed that the downstream tasks can be divided into some groups, whereas each set in a group can be represented as a pattern-verbalizer pair. After pre-training soft prompts on these tasks with fixed model parameters, some pre-trained prompts were obtained and soft prompts initialization was used to optimize the whole process for each task. <br> Then the typical classification tasks are grouped into three formats, i.e, sentence-pair classification, multiple-choice classification, and single-text classification to ensure the generalization of pre-trained prompts. In addition, multiple-choice classification has been found to be more general among these formats and all classification tasks can be unified to this format. |  
| Results and Findings| In their research, they tested two variants of PPT: (1) Hybrid PPT, where carefully designed hard prompts are combined with pre-trained soft prompt and (2) Unified PPT, that takes unified tasks in the multiple-choice classification format. <br> However, the major findings from their experiments are: <br> 1. Since larger models achieve better overall performance under the few-shot setting, PT was studied on the large-scale pre-trained model. The experimental results showed that CPM-2 was able to outperform mT5-XXL across all tasks on Chinese datasets though they share the same parameter scale. <br> 2. PPT was capable of performing better than Vanilla PT and LM Adaption on most datasets significantly. Though it performed worse than Hybrid PT on BoolQ, using Hybrid PPT by combining PPT and hard prompts could outperform all baselines. Experimenting on RACE-m, LCQMC and C3 datasets also provided similar observation. <br> 3. PPT could outperform FT on all Chinese datasets and most English datasets, indicating the existing gap between masked language modeling and downstream tasks. Based on this observation, they intended to extend the proposed method in future. <br> 4. PPT resulted in lower variances across all the datasets with the help of additional hard prompts or further pre-training with language modeling whereas Vanilla PT or Hybrid PT increased the variances for some datasets which is not different from random guesses. <br> Moreover, they tested Unified PPT on datasets with more than 5 labels, whereas it outperformed FT and PT baselines by a large margin. Futhermore, they observed that PPT was able to speed up the convergence of Vanilla PT on RACE-m and CB datasets though it converged a bit slower than FT. | 
| Limitations | There is still an existing gap between masked language modeling and downstream tasks though prompt pre-training was capable to bridge this gap to some extent. Moreover, PPT converges slower than FT. Therefore, how to further accelerate the convergence needs to explored in future according to this research. |  
| Future Work | The significant future research directions based on this paper are: (1) designing unified task formats with their corresponding pre-training objectives for different tasks such as, language generation or relation extraction (2) evaluating the performance of few-shot learning techniques considering other parameter tuning approaches and adapting unified task pre-training to them and (3) studying the performance of the unified task pre-training on pre-trained language models apart the soft-prompt based approaches. | 
